{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-10T09:10:16.249655Z",
     "iopub.status.busy": "2025-05-10T09:10:16.249110Z",
     "iopub.status.idle": "2025-05-10T09:10:16.594046Z",
     "shell.execute_reply": "2025-05-10T09:10:16.593241Z",
     "shell.execute_reply.started": "2025-05-10T09:10:16.249625Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra?\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T09:10:16.596125Z",
     "iopub.status.busy": "2025-05-10T09:10:16.595385Z",
     "iopub.status.idle": "2025-05-10T09:10:16.937870Z",
     "shell.execute_reply": "2025-05-10T09:10:16.936934Z",
     "shell.execute_reply.started": "2025-05-10T09:10:16.596099Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/sentiment140\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T09:10:16.938906Z",
     "iopub.status.busy": "2025-05-10T09:10:16.938679Z",
     "iopub.status.idle": "2025-05-10T09:10:23.452721Z",
     "shell.execute_reply": "2025-05-10T09:10:23.452145Z",
     "shell.execute_reply.started": "2025-05-10T09:10:16.938882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\",encoding=\"latin-1\",header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T09:10:29.342886Z",
     "iopub.status.busy": "2025-05-10T09:10:29.342594Z",
     "iopub.status.idle": "2025-05-10T09:10:29.635753Z",
     "shell.execute_reply": "2025-05-10T09:10:29.635114Z",
     "shell.execute_reply.started": "2025-05-10T09:10:29.342854Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "1    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns=[\"target\",\"ids\",\"date\",\"flag\",\"user\",\"text\"]\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.drop([\"ids\",\"date\",\"flag\",\"user\"],axis=1,inplace=True)\n",
    "\n",
    "df['target']=df['target'].replace(4,1)\n",
    "\n",
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T09:10:29.637125Z",
     "iopub.status.busy": "2025-05-10T09:10:29.636573Z",
     "iopub.status.idle": "2025-05-10T09:10:29.654858Z",
     "shell.execute_reply": "2025-05-10T09:10:29.654208Z",
     "shell.execute_reply.started": "2025-05-10T09:10:29.637097Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  is upset that he can't update his Facebook by ...\n",
       "2       0  @Kenichan I dived many times for the ball. Man...\n",
       "3       0    my whole body feels itchy and like its on fire \n",
       "4       0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T09:10:29.655892Z",
     "iopub.status.busy": "2025-05-10T09:10:29.655627Z",
     "iopub.status.idle": "2025-05-10T09:10:30.703681Z",
     "shell.execute_reply": "2025-05-10T09:10:30.702901Z",
     "shell.execute_reply.started": "2025-05-10T09:10:29.655861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "# ... (rest of your code) ...\n",
    "\n",
    "def clean_text(tweet):\n",
    "    # Check if the input is a string, convert if not\n",
    "    if not isinstance(tweet, str):\n",
    "        tweet = str(tweet)\n",
    "    \n",
    "    tweet= re.sub('(#|@)\\w*','',tweet)\n",
    "    tweet= re.sub('https?:\\/\\/\\S+','',tweet)\n",
    "    tweet= re.sub('(\\?|\\!)+','',tweet)\n",
    "    tweet= re.sub('^\\s+','',tweet)\n",
    "    tweet= re.sub('\\s+$','',tweet)\n",
    "    tweet= re.sub('(\\.|\\,)+','',tweet)\n",
    "    tweet= re.sub('[:()\\\\\\]','',tweet)\n",
    "    tweet= re.sub('\\s\\d+\\s','',tweet)\n",
    "    tweet = tweet.lower()  \n",
    "    tweet = re.sub(r\"\\d+\", \"\", tweet) \n",
    "    tweet = tweet.translate(str.maketrans(\"\", \"\", string.punctuation)) \n",
    "    tweet = tweet.strip()\n",
    "    return tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T09:10:30.705077Z",
     "iopub.status.busy": "2025-05-10T09:10:30.704633Z",
     "iopub.status.idle": "2025-05-10T09:11:01.754685Z",
     "shell.execute_reply": "2025-05-10T09:11:01.753877Z",
     "shell.execute_reply.started": "2025-05-10T09:10:30.705044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df[\"text\"] = df['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T09:11:01.755876Z",
     "iopub.status.busy": "2025-05-10T09:11:01.755653Z",
     "iopub.status.idle": "2025-05-10T09:11:04.345698Z",
     "shell.execute_reply": "2025-05-10T09:11:04.345094Z",
     "shell.execute_reply.started": "2025-05-10T09:11:01.755859Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T09:11:04.348505Z",
     "iopub.status.busy": "2025-05-10T09:11:04.348159Z",
     "iopub.status.idle": "2025-05-10T09:11:05.973105Z",
     "shell.execute_reply": "2025-05-10T09:11:05.972519Z",
     "shell.execute_reply.started": "2025-05-10T09:11:04.348463Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0d2640504641a9a5ac054b647712e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea804716c994cdba1bfaaabc1cdd968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52bdca572ef439ebbe32ac79a6cbede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c6acf9042a4f12b7b5e5f4613ef5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ✅ تحميل Tokenizer الخاص بـ RoBERTa\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# ✅ دالة تحويل النصوص إلى توكينات\n",
    "def tokenize_texts(texts, max_length=128):\n",
    "    encoding = tokenizer(\n",
    "        texts.tolist(),\n",
    "        \n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T09:11:05.974091Z",
     "iopub.status.busy": "2025-05-10T09:11:05.973864Z",
     "iopub.status.idle": "2025-05-10T09:13:51.747647Z",
     "shell.execute_reply": "2025-05-10T09:13:51.746973Z",
     "shell.execute_reply.started": "2025-05-10T09:11:05.974073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ✅ تحويل النصوص إلى توكينات\n",
    "\n",
    "input_ids, attention_masks = tokenize_texts(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T09:13:51.749559Z",
     "iopub.status.busy": "2025-05-10T09:13:51.748958Z",
     "iopub.status.idle": "2025-05-10T09:13:51.913723Z",
     "shell.execute_reply": "2025-05-10T09:13:51.913013Z",
     "shell.execute_reply.started": "2025-05-10T09:13:51.749527Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "labels = torch.tensor(df[\"target\"].values)\n",
    "\n",
    "\n",
    "# ✅ تنظيف الذاكرة\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T09:13:51.914807Z",
     "iopub.status.busy": "2025-05-10T09:13:51.914541Z",
     "iopub.status.idle": "2025-05-10T09:13:53.245606Z",
     "shell.execute_reply": "2025-05-10T09:13:53.244808Z",
     "shell.execute_reply.started": "2025-05-10T09:13:51.914778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ✅ تقسيم البيانات إلى تدريب واختبار\n",
    "X_train, X_test, y_train, y_test, mask_train, mask_test = train_test_split(\n",
    "    input_ids, labels, attention_masks, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ إنشاء Dataset مخصص\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_masks[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# ✅ إنشاء DataLoader\n",
    "batch_size = 16  # اختبر قيمًا أكبر لو كان الأداء جيدًا\n",
    "train_dataset = SentimentDataset(X_train, mask_train, y_train)\n",
    "test_dataset = SentimentDataset(X_test, mask_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T09:13:53.246725Z",
     "iopub.status.busy": "2025-05-10T09:13:53.246457Z",
     "iopub.status.idle": "2025-05-10T09:13:53.250604Z",
     "shell.execute_reply": "2025-05-10T09:13:53.249953Z",
     "shell.execute_reply.started": "2025-05-10T09:13:53.246707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T09:21:05.781855Z",
     "iopub.status.busy": "2025-05-10T09:21:05.781170Z",
     "iopub.status.idle": "2025-05-10T09:21:05.786234Z",
     "shell.execute_reply": "2025-05-10T09:21:05.785407Z",
     "shell.execute_reply.started": "2025-05-10T09:21:05.781827Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T09:26:11.619948Z",
     "iopub.status.busy": "2025-05-10T09:26:11.619006Z",
     "iopub.status.idle": "2025-05-10T09:26:12.029045Z",
     "shell.execute_reply": "2025-05-10T09:26:12.028513Z",
     "shell.execute_reply.started": "2025-05-10T09:26:11.619918Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ✅ تحميل نموذج RoBERTa جاهز للتصنيف\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "# ✅ إعداد الـ Optimizer & Loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T09:26:13.690519Z",
     "iopub.status.busy": "2025-05-10T09:26:13.690199Z",
     "iopub.status.idle": "2025-05-10T09:26:13.698558Z",
     "shell.execute_reply": "2025-05-10T09:26:13.697810Z",
     "shell.execute_reply.started": "2025-05-10T09:26:13.690495Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, num_epochs=3, patience=2,\n",
    "             load_checkpoint_path=\"checkpoint.pt\", save_checkpoint_path=\"checkpoint_new.pt\"):\n",
    "    model.to(device)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    counter = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    # ✅ لو في Checkpoint محفوظ، نحمله من ملف مختلف\n",
    "    if os.path.exists(load_checkpoint_path):\n",
    "        checkpoint = torch.load(load_checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_loss = checkpoint['best_loss']\n",
    "        print(f\"✅ تم استعادة التدريب من Epoch {start_epoch}\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"📌 Epoch {epoch+1}: Average Loss = {avg_loss:.4f}, Time = {(time.time() - start_time) / 60:.2f} دقيقة\")\n",
    "\n",
    "        # ✅ حفظ Checkpoint في فايل جديد\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "        }, save_checkpoint_path)\n",
    "        print(f\"💾 تم حفظ Checkpoint عند Epoch {epoch+1} في {save_checkpoint_path}\")\n",
    "\n",
    "        # ✅ Early Stopping\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"⏳ Early Stopping: No improvement, stopping training.\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-10T18:52:30.506Z",
     "iopub.execute_input": "2025-05-10T09:26:16.414982Z",
     "iopub.status.busy": "2025-05-10T09:26:16.414698Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 80000/80000 [8:32:23<00:00,  2.60it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Epoch 1: Average Loss = 0.3376, Time = 512.39 دقيقة\n",
      "💾 تم حفظ Checkpoint عند Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  10%|█         | 8398/80000 [53:48<7:39:45,  2.60it/s]"
     ]
    }
   ],
   "source": [
    "train_fn(train_loader, model, optimizer, num_epochs=2, patience=2, checkpoint_path=\"checkpoint.pt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-10T06:18:22.950183Z",
     "iopub.status.idle": "2025-05-10T06:18:22.950495Z",
     "shell.execute_reply": "2025-05-10T06:18:22.950355Z",
     "shell.execute_reply.started": "2025-05-10T06:18:22.950340Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# ✅ حفظ النموذج\n",
    "model_save_path = \"roberta_sentiment_model.pt\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# ✅ إذا كنت في Kaggle أو Colab، قم بنقل الملف إلى مجلد العمل\n",
    "shutil.move(model_save_path, \"/kaggle/working/roberta_sentiment_model.pt\")  # لكاجل\n",
    "\n",
    "\n",
    "print(f\"✅ تم حفظ النموذج في {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-10T06:18:22.951329Z",
     "iopub.status.idle": "2025-05-10T06:18:22.951532Z",
     "shell.execute_reply": "2025-05-10T06:18:22.951442Z",
     "shell.execute_reply.started": "2025-05-10T06:18:22.951434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# إنشاء مجلد لحفظ النموذج\n",
    "model_dir = \"/kaggle/working/roberta_trained\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# ✅ حفظ النموذج والتوكينايزر\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "print(f\"✅ تم حفظ النموذج والتوكينايزر في: {model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-10T06:18:22.952260Z",
     "iopub.status.idle": "2025-05-10T06:18:22.952593Z",
     "shell.execute_reply": "2025-05-10T06:18:22.952443Z",
     "shell.execute_reply.started": "2025-05-10T06:18:22.952427Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# ✅ تحويل الموديل إلى وضع التقييم\n",
    "model.eval()\n",
    "\n",
    "# ✅ تخزين التوقعات والملصقات الحقيقية\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# ✅ عدم احتساب الجريدينت أثناء التقييم\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-10T06:18:22.954138Z",
     "iopub.status.idle": "2025-05-10T06:18:22.954381Z",
     "shell.execute_reply": "2025-05-10T06:18:22.954279Z",
     "shell.execute_reply.started": "2025-05-10T06:18:22.954266Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"🎯 دقة النموذج: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-10T06:18:22.955207Z",
     "iopub.status.idle": "2025-05-10T06:18:22.955516Z",
     "shell.execute_reply": "2025-05-10T06:18:22.955375Z",
     "shell.execute_reply.started": "2025-05-10T06:18:22.955360Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"📊 تقرير الأداء:\\n\")\n",
    "print(classification_report(true_labels, predictions, target_names=[\"neg\", \"natural\", \"pos\"]))  # استبدل الفئات بأسماء التصنيفات الفعلية لديك\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-10T06:18:22.956871Z",
     "iopub.status.idle": "2025-05-10T06:18:22.957230Z",
     "shell.execute_reply": "2025-05-10T06:18:22.957085Z",
     "shell.execute_reply.started": "2025-05-10T06:18:22.957052Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "labels = [\"neg\", \"natural\", \"pos\"]  # استبدلها بالفئات الصحيحة لديك\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"pred\")\n",
    "plt.ylabel(\"true\")\n",
    "plt.title(\"📊 confusion metrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-10T06:18:22.959177Z",
     "iopub.status.idle": "2025-05-10T06:18:22.959462Z",
     "shell.execute_reply": "2025-05-10T06:18:22.959353Z",
     "shell.execute_reply.started": "2025-05-10T06:18:22.959341Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# لو كنت لسه ما حمّلتش الـ model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# تحميل الـ checkpoint\n",
    "checkpoint = torch.load(\"/kaggle/working/checkpoint.pt\")  # أو المسار المناسب عندك\n",
    "\n",
    "# استرجاع الحالة\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "best_loss = checkpoint['best_loss']\n",
    "\n",
    "print(f\"✅ تم استعادة النموذج من epoch رقم {start_epoch} - أفضل Loss: {best_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2477,
     "sourceId": 4140,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
